#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ParaView/VTKæ•°æ®æå–è„šæœ¬
ä».eæ–‡ä»¶ä¸­æå–æŒ‡å®šèŠ‚ç‚¹/å•å…ƒçš„æ•°æ®å¹¶å¯¼å‡ºåˆ°Excel
"""

import os
import sys
import subprocess
import numpy as np
import pandas as pd

# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“
import matplotlib.pyplot as plt
plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False

# ================== é…ç½®å‚æ•° ==================
# è¯·æ ¹æ®éœ€è¦ä¿®æ”¹ä»¥ä¸‹å‚æ•°

# æ•°æ®æ–‡ä»¶è·¯å¾„
DATA_FILE = "elastoplasticity_out_1.5.e"

# è¦æå–çš„èŠ‚ç‚¹IDåˆ—è¡¨
NODE_IDS = [1, 10, 50, 100, 200, 500]

# è¦æå–çš„å•å…ƒIDåˆ—è¡¨  
CELL_IDS = [1, 10, 50, 100, 200, 500]

# æ—¶é—´èŒƒå›´ [å¼€å§‹æ—¶é—´, ç»“æŸæ—¶é—´]
TIME_RANGE = [0.0, 1000.0]  # å¢å¤§æ—¶é—´èŒƒå›´ï¼Œè·å–åæœŸçš„å¡‘æ€§åº”å˜æ•°æ®

# è¦æå–çš„å­—æ®µåˆ—è¡¨ [å­—æ®µå, æ•°æ®ç±»å‹]
# æ•°æ®ç±»å‹: 'point' æˆ– 'cell'
FIELD_LIST = [
    ('d', 'point'),                      # ç›¸åœºå˜é‡
    ('effective_plastic_strain', 'cell'), # æœ‰æ•ˆå¡‘æ€§åº”å˜
]

# è¾“å‡ºExcelæ–‡ä»¶å
OUTPUT_FILE = "extracted_data.xlsx"

# ================== æ•°æ®è¯»å–æ¨¡å— ==================

def extract_data_from_exodus(file_path):
    """ä»Exodusæ–‡ä»¶ä¸­æå–æ•°æ®"""
    try:
        from paraview.simple import (
            OpenDataFile, UpdatePipeline, GetAnimationScene, MergeBlocks
        )
        import paraview.simple as pv
        import paraview.servermanager as sm
        pv._DisableFirstRenderCameraReset()
        
        # å¯¼å…¥vtkç”¨äºæ•°æ®å¤„ç†
        import vtk
        from vtk.util import numpy_support
        
    except ImportError as e:
        print(f"âŒ æ— æ³•å¯¼å…¥ParaViewæ¨¡å—: {e}")
        print("è¯·ç¡®ä¿ParaViewç¯å¢ƒå·²æ­£ç¡®é…ç½®")
        return None

    try:
        # 1. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(file_path):
            print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return None
            
        # 2. è¯»å–æ–‡ä»¶
        print(f"ğŸ“– æ­£åœ¨è¯»å–æ–‡ä»¶: {os.path.basename(file_path)}")
        reader = pv.OpenDataFile(file_path)
        if not reader:
            print("âŒ æ— æ³•è¯»å–æ–‡ä»¶")
            return None

        # 3. è·å–æ—¶é—´æ­¥ä¿¡æ¯
        time_steps = []
        if hasattr(reader, 'TimestepValues'):
            time_steps = [float(t) for t in reader.TimestepValues]
        elif hasattr(reader, 'TimestepValue'):
            time_steps = [float(reader.TimestepValue)]
        
        if not time_steps:
            print("âš ï¸  æœªæ‰¾åˆ°æ—¶é—´æ­¥ä¿¡æ¯")
            return None

        print(f"â±ï¸  æ‰¾åˆ° {len(time_steps)} ä¸ªæ—¶é—´æ­¥")
        print(f"â±ï¸  æ—¶é—´èŒƒå›´: {min(time_steps):.2e} - {max(time_steps):.2e}")

        # 4. ç¡®å®šè¦æå–çš„æ—¶é—´ç‚¹
        start_time, end_time = TIME_RANGE
        target_times = [t for t in time_steps if start_time <= t <= end_time]
        
        if not target_times:
            print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ—¶é—´ç‚¹")
            return None

        print(f"ğŸ“Š å°†æå– {len(target_times)} ä¸ªæ—¶é—´ç‚¹çš„æ•°æ®")

        # 5. åˆå§‹åŒ–æ•°æ®å­˜å‚¨
        extracted_data = {}
        
        # 6. å¤„ç†å¤šå—æ•°æ®é›† - ä½¿ç”¨MergeBlocks
        print("ğŸ”„ åˆå¹¶å¤šå—æ•°æ®é›†...")
        merged_data = MergeBlocks(Input=reader)
        pv.UpdatePipeline(proxy=merged_data)
        
        # ä½¿ç”¨æ­£ç¡®çš„APIè·å–æ•°æ®
        data_object = sm.Fetch(merged_data)
        
        if not data_object:
            print("âŒ æ— æ³•è·å–æ•°æ®å¯¹è±¡")
            return None

        # 7. æ£€æŸ¥å¯ç”¨å­—æ®µ
        point_arrays = []
        cell_arrays = []
        
        point_data = data_object.GetPointData()
        for i in range(point_data.GetNumberOfArrays()):
            array_name = point_data.GetArrayName(i)
            if array_name:
                point_arrays.append(array_name)
        
        cell_data = data_object.GetCellData()
        for i in range(cell_data.GetNumberOfArrays()):
            array_name = cell_data.GetArrayName(i)
            if array_name:
                cell_arrays.append(array_name)

        print(f"ğŸ“‹ å¯ç”¨ç‚¹æ•°æ®å­—æ®µ: {point_arrays}")
        print(f"ğŸ“‹ å¯ç”¨å•å…ƒæ•°æ®å­—æ®µ: {cell_arrays}")

        # 8. å¤„ç†æ¯ä¸ªæ—¶é—´æ­¥
        animationScene = pv.GetAnimationScene()
        
        for time_idx, target_time in enumerate(target_times):
            print(f"\nğŸ”„ å¤„ç†æ—¶é—´æ­¥ {time_idx+1}/{len(target_times)}: {target_time:.2e}")
            
            # è®¾ç½®æ—¶é—´æ­¥
            animationScene.AnimationTime = float(target_time)
            pv.UpdatePipeline(time=target_time, proxy=merged_data)
            
            # è·å–å½“å‰æ—¶é—´æ­¥çš„æ•°æ®
            current_data = sm.Fetch(merged_data)
            
            # 9. æå–æ¯ä¸ªå­—æ®µçš„æ•°æ®
            for field_name, data_type in FIELD_LIST:
                if field_name not in extracted_data:
                    extracted_data[field_name] = {
                        'times': [],
                        'data': {},
                        'type': data_type
                    }
                
                # è®°å½•æ—¶é—´
                if target_time not in extracted_data[field_name]['times']:
                    extracted_data[field_name]['times'].append(target_time)
                
                # è·å–å­—æ®µæ•°æ®
                if data_type == 'point':
                    if field_name not in point_arrays:
                        print(f"  âš ï¸  ç‚¹æ•°æ®å­—æ®µ '{field_name}' ä¸å­˜åœ¨")
                        continue
                    
                    data_array = current_data.GetPointData().GetArray(field_name)
                    if not data_array:
                        print(f"  âš ï¸  æ— æ³•è·å–ç‚¹æ•°æ® '{field_name}'")
                        continue
                    
                    # è½¬æ¢ä¸ºnumpyæ•°ç»„
                    np_array = numpy_support.vtk_to_numpy(data_array)
                    
                    # æå–æŒ‡å®šIDçš„æ•°æ®
                    for node_id in NODE_IDS:
                        if node_id < len(np_array):
                            if node_id not in extracted_data[field_name]['data']:
                                extracted_data[field_name]['data'][node_id] = []
                            extracted_data[field_name]['data'][node_id].append(np_array[node_id])
                        else:
                            print(f"  âš ï¸  èŠ‚ç‚¹ID {node_id} è¶…å‡ºèŒƒå›´")
                
                elif data_type == 'cell':
                    if field_name not in cell_arrays:
                        print(f"  âš ï¸  å•å…ƒæ•°æ®å­—æ®µ '{field_name}' ä¸å­˜åœ¨")
                        continue
                    
                    data_array = current_data.GetCellData().GetArray(field_name)
                    if not data_array:
                        print(f"  âš ï¸  æ— æ³•è·å–å•å…ƒæ•°æ® '{field_name}'")
                        continue
                    
                    # è½¬æ¢ä¸ºnumpyæ•°ç»„
                    np_array = numpy_support.vtk_to_numpy(data_array)
                    
                    # æ·»åŠ è°ƒè¯•ä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯å¯¹effective_plastic_strain
                    if field_name == 'effective_plastic_strain':
                        non_zero_count = np.count_nonzero(np_array)
                        print(f"  ğŸ“Š {field_name}: éé›¶å€¼æ•°é‡ = {non_zero_count}/{len(np_array)}, max = {np_array.max():.8f}")
                        if non_zero_count > 0:
                            # æ‰¾å‡ºæœ‰éé›¶å€¼çš„å•å…ƒID
                            non_zero_indices = np_array.nonzero()[0]
                            print(f"    éé›¶å€¼å•å…ƒID: {non_zero_indices[:5]}...")
                    
                    # æå–æŒ‡å®šIDçš„æ•°æ®
                    for cell_id in CELL_IDS:
                        if cell_id < len(np_array):
                            if cell_id not in extracted_data[field_name]['data']:
                                extracted_data[field_name]['data'][cell_id] = []
                            value = np_array[cell_id]
                            extracted_data[field_name]['data'][cell_id].append(value)
                            
                            # æ·»åŠ è°ƒè¯•ä¿¡æ¯
                            if field_name == 'effective_plastic_strain' and value != 0:
                                print(f"    å•å…ƒID {cell_id}: {value:.8f}")
                        else:
                            print(f"  âš ï¸  å•å…ƒID {cell_id} è¶…å‡ºèŒƒå›´")

        return extracted_data

    except Exception as e:
        print(f"âŒ æ•°æ®æå–å¤±è´¥: {str(e)}")
        return None

def export_to_excel(data, output_file):
    """å¯¼å‡ºæ•°æ®åˆ°Excelæ–‡ä»¶"""
    try:
        print(f"\nğŸ“Š æ­£åœ¨å¯¼å‡ºæ•°æ®åˆ°Excel...")
        
        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
            
            for field_name, field_data in data.items():
                print(f"  ğŸ“ å¯¼å‡ºå­—æ®µ: {field_name}")
                
                # è·å–æ—¶é—´åˆ—è¡¨
                times = sorted(field_data['times'])
                
                # åˆ›å»ºDataFrame
                df_data = {'Time': times}
                
                # æ·»åŠ æ¯ä¸ªIDçš„æ•°æ®
                for id_val, values in field_data['data'].items():
                    if len(values) == len(times):
                        df_data[f'ID_{id_val}'] = values
                    else:
                        print(f"    âš ï¸  ID {id_val} çš„æ•°æ®é•¿åº¦ä¸åŒ¹é…")
                
                # åˆ›å»ºDataFrameå¹¶å†™å…¥Excel
                df = pd.DataFrame(df_data)
                sheet_name = field_name[:31]  # Excelå·¥ä½œè¡¨åé•¿åº¦é™åˆ¶
                df.to_excel(writer, sheet_name=sheet_name, index=False)
                
                print(f"    âœ… å·¥ä½œè¡¨ '{sheet_name}' å·²åˆ›å»ºï¼Œæ•°æ®å½¢çŠ¶: {df.shape}")
        
        print(f"\nâœ… æ•°æ®å·²æˆåŠŸå¯¼å‡ºåˆ°: {output_file}")
        return True
        
    except Exception as e:
        print(f"âŒ Excelå¯¼å‡ºå¤±è´¥: {str(e)}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸš€ ParaViewæ•°æ®æå–å·¥å…·")
    print("=" * 60)
    
    print("ğŸ” å¼€å§‹æ•°æ®æå–...")
    
    print(f"ğŸ“ æ•°æ®æ–‡ä»¶: {DATA_FILE}")
    print(f"ğŸ”¢ èŠ‚ç‚¹ID: {NODE_IDS}")
    print(f"ğŸ”¢ å•å…ƒID: {CELL_IDS}")
    print(f"â±ï¸  æ—¶é—´èŒƒå›´: {TIME_RANGE}")
    print(f"ğŸ“‹ å­—æ®µåˆ—è¡¨: {FIELD_LIST}")
    print(f"ğŸ“Š è¾“å‡ºæ–‡ä»¶: {OUTPUT_FILE}")
    
    # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•
    script_dir = os.path.dirname(os.path.abspath(__file__))
    data_file_path = os.path.join(script_dir, DATA_FILE)
    output_file_path = os.path.join(script_dir, OUTPUT_FILE)
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(data_file_path):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file_path}")
        return
    
    # æå–æ•°æ®
    extracted_data = extract_data_from_exodus(data_file_path)
    
    if extracted_data:
        # å¯¼å‡ºåˆ°Excel
        if export_to_excel(extracted_data, output_file_path):
            print("\nğŸ‰ æ•°æ®æå–å’Œå¯¼å‡ºå®Œæˆ!")
            print(f"ğŸ“‚ è¾“å‡ºæ–‡ä»¶ä½ç½®: {output_file_path}")
        else:
            print("\nâŒ æ•°æ®å¯¼å‡ºå¤±è´¥")
    else:
        print("\nâŒ æ•°æ®æå–å¤±è´¥")

if __name__ == "__main__":
    main() 